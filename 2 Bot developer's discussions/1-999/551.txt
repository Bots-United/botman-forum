--------------------------------------------------
Subject: BOT SENSES - VISON
--------------------------------------------------
11/07/00 at 15:50:54  Posted by: FuckFace (gellert.lukacs@dbs.hu)
--------------------------------------------------
foreword at BOT SENSES - IDEAS topic

The most advenced 'already implemented' part in BOTS is vision. However it still needs a lot of improovemnets. Let's see what would be recorded for the Vision class when a bot 'looks around'

- it obviously determines if it can see an enemy.
- it would also be very helpfull to determine if the 'enemy' can see the bot, which should allow the bot to decide if it is neccessary to attack etc... it also helps movement prediction (later).
- grenades must be detected
- light level should be considered (local and 'enemy remote'
- flashbang events and smoke grenades must be taken into account.
- 'interesting entities' should be recorded.

and maybe a lot more.

the point is, that after this 'look around' call, the bot will have some info about the environment it operates in. After we gathered all the info, the BOT can USE it as it likes.

From a programmer's eye, this classification allows that not all part's must be implemented at once. It the bot can't see a grenade - sorry, if it is not blinded by a flasbang - sorry for the enemy. BUT: the BOT mechanics can depend on only ONE interface, which if changed can be easily replaced.

don't forget: many of the above requirements may have already been implemented (most of it) and must have been a hard work. lets put it together.







--------------------------------------------------
11/08/00 at 01:24:38  Reply by: William (william@botepidemic.com)
--------------------------------------------------
For more ideas on vision, have a look at:

http://www.botepidemic.com/aid/cgf/design_vision.shtml

It introduces tracking, and relates vision and aiming (something that a few CS bots could benefit from).

Note that your proposal is a push model: first collect all the info that's available, then have the AI use it.

In general, vision information requires ray tracing in the BSP, and isn't very cheap. It is more appropriate to use a pull model: have the bot indicate what (subset) information it could 'consume' in its next thinking frame.

In addition to flashbangs, vision should be degraded when being hit. There's a limit to how many things a bot can track at a given time. Zooming is to be included, as should be transparent textures (glass, water, but also semi-transparent textures such as camouflage nets (in CS aztec map?)).
Don't forget about flash lights, muzzles flashes...

William

